---
title: "class 9: Unsupervised learning mini-project"
author: "Anusorn Mudla"
date: "5/1/2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Unsupervised Machine Learning

```{r}
# read the data
cancer <- read.csv("WisconsinCancer.csv")
#Q1.. What type of object is returned from the read.csv() function? 
class(cancer)
#Q2. How many observations (i.e. patients) are in this dataset?
dim(cancer)
nrow(cancer)
#Q3. How many of the observations have a malignant diagnosis?
table(cancer$diagnosis)
#Q4. How many variables/features in the data are suffixed with _mean?
#length(grep("_mean",dimnames(cancer)[[2]]))
length(grep("_mean",colnames(cancer)))
```
```{r}
wisc.data <- as.matrix(cancer[,3:32])
#Q5.. Why do you think we are using the indices 3:32 here?
# the last colunm has NA values and we don't want the first two column because they are not measurements

row.names(wisc.data) <- cancer$id
diagnosis <- cancer$diagnosis

```
#Perform PCA
```{r}
colMeans(wisc.data)
apply(wisc.data,2,mean)
# the means are very different so we need to rescale the data

wisc.pr <- prcomp(wisc.data,scale = TRUE)
summary(wisc.pr)
```

```{r}
biplot(wisc.pr)
```

```{r}
plot(x=wisc.pr$x[,1],y = wisc.pr$x[,2],col = diagnosis)
```
```{r}
plot(x=wisc.pr$x[,1],y = wisc.pr$x[,7],col = diagnosis)
```
```{r}
pr.var <- wisc.pr$sdev^2
pve <- pr.var/sum(pr.var)

# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```
```{r}
# Alternative scree plot of the same data, note data driven y-axis
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 ) # axis(what axis (x or y),at =s data)
```
```{r}
sort(abs(wisc.pr$rotation[,1]))
```
## Using factoextra package
```{r}
library(factoextra)
library(ggplot2)
```

## Hierarchical clustering
```{r}
#Scale the wisc.data data and assign the result to data.scaled
data.scaled <- scale(wisc.data)
# ? what distance is measuring
data.dist <- dist(data.scaled)
wisc.hclust <- hclust(data.dist)
plot(wisc.hclust)
abline(h=19,col="red",lty=2)
```

```{r}
tree <- cutree(wisc.hclust,h=19)
tree <- cutree(wisc.hclust,k=4)
table(tree)
```
```{r}
table(tree,diagnosis)

```
## Section 4: K-means clustering
```{r}
wisc.km <- kmeans(scale(wisc.data),centers = 2, nstart = 20)
table(wisc.km$cluster,diagnosis)
```

Section 5: Combining Methods (PCA + hclust)
We will need 7 PC to capture more than 90% of the varinace in the data
```{r}
wisc.pr.hclust <- hclust(dist(wisc.pr$x[,1:7]),method = "ward.D2")
plot(wisc.pr.hclust)
```

```{r}
groups <- cutree(wisc.pr.hclust,k=2)
table(groups)
table(groups,diagnosis)
```
```{r}
plot(x=wisc.pr$x[,1],y=wisc.pr$x[,2],col=as.factor(diagnosis))
```
# plot 3D plot
```{r}
library(rgl)
plot3d(wisc.pr$x[,1:3], xlab="PC 1", ylab="PC 2", zlab="PC 3", cex=1.5, size=1, type="s", col=groups)
```

## Section 7
Use the model to predict the new data set
```{r}
new_sample <- read.csv("new_samples.csv")
npc <- predict(wisc.pr,newdata = new_sample)
npc
```
```{r}
mycols <- rep("green", length(diagnosis))
mycols[ diagnosis=="M" ] <- "red"
plot(wisc.pr$x[,1:2], col=mycols)
points(npc[,1], npc[,2], col=c("green","red"), pch=16, cex=3)
text(npc[,1], npc[,2], c(1,2), col="black")
```




