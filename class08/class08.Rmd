---
title: "Class08: Introduction"
author: "Anusorn Mudla"
date: "4/26/2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Class08
Today we will go over machine learning in R (unsupervised learning)

## k-mean clustering
kmean(x=data,centers = 3,nstarts = 20) # will generate 3 clusters and iterate 20 times to find the lowerest variances between data...looking for lowest variance. R always returns 3 clusters as you set the "centers" argument. However, sometimes what you set for the number of clusters is not good for the data. The systematic method to determine the number of cluster is to plot scree plot (k vs ss within) to determine the best number of cluster.

```{r}
 # Generate some example data for clustering
tmp <- c(rnorm(30,-3), rnorm(30,3))
x <- cbind(x=tmp, y=rev(tmp))
plot(x)
```
k Use the kmeans() function setting k to 2 and nstart=20
```{r}
km <- kmeans(x,centers = 2,nstart = 20)
```

Inspect/print the results
Q. How many points are in each cluster?
```{r}
table(km$cluster)
```

Q. What ‘component’ of your result object details
      - cluster size?
```{r}
km$size
```
      
      - cluster assignment/membership?
```{r}
km$cluster
```
      
      - cluster center?
```{r}
km$centers
```
      
Plot x colored by the kmeans cluster assignment and
      add cluster centers as blue points
```{r}
plot(x,col=km$cluster)
points(km$centers,col="blue",pch =3,cex=3)

```
      
## Hierachical Clustering

# First we need to calculate point (dis)similarity as the Euclidean distance between observations
```{r}
dist_matrix <- dist(x)
```
# The hclust() function returns a hierarchical clustering model
```{r}
hc <- hclust(d = dist_matrix)
```
# the print method is not so useful here so we need to plot it
```{r}
plot(hc)
abline(h=6,col="red") # draw a line at height = 6
cutree(hc,h=6) # we can also do cutree(hc,k=2) to specify the number of clusters
gp2 <- cutree(hc,k=2)
gp3 <- cutree(hc,k=3)
table(gp2,gp3)
```

```{r}
 # Step 1. Generate some example data for clustering
x <- rbind(
  matrix(rnorm(100, mean=0, sd = 0.3), ncol = 2),   # c1
  matrix(rnorm(100, mean = 1, sd = 0.3), ncol = 2), # c2
  matrix(c(rnorm(50, mean = 1, sd = 0.3),           # c3
           rnorm(50, mean = 0, sd = 0.3)), ncol = 2))
colnames(x) <- c("x", "y")
# Step 2. Plot the data without clustering
plot(x)
# Step 3. Generate colors for known clusters
#         (just so we can compare to hclust results)
col <- as.factor( rep(c("c1","c2","c3"), each=50) )
plot(x, col=col)
```

```{r}
hc2 <- hclust(dist(x))
plot(hc2)
gp2<- cutree(hc2,k=2)
gp3 <- cutree(hc2,k=3)
table(gp2,gp3)
plot(hc2)

#can we color the dendalion plot

```

## PCA analysis

```{r}
## You can also download this file from the class website!
mydata <- read.csv("https://tinyurl.com/expression-CSV",row.names=1)
```
There are `r nrow(mydata)` gene in this dataset
```{r}
#transpose the dataset with t()
pca <- prcomp(t(mydata),scale=TRUE)
summary(pca)
```
```{r}
attributes(pca)
```
```{r}
plot(x=pca$x[,1],y=pca$x[,2])
## Variance captured per PC
pca.var <- pca$sdev^2
## Precent variance is often more informative to look at
pca.var <- pca$sdev^2
pca.var.per <- round(pca.var/sum(pca.var)*100, 1)
pca.var.per
```
```{r}
col <- c(rep("red",5),rep("black",5))
plot(x=pca$x[,1],y=pca$x[,2],xlab=paste("PC1 (",pca.var.per[1],"%)", sep = ""),
                             ylab = paste("PC2 (",pca.var.per[2],"%)", sep = ""),col = col)
text(pca$x[,1],pca$x[2])
# identify(x=pca$x[,1],y=pca$x[,2],colnames(mydata))
```
what gene contribute the most to PC1
```{r}
gene_score <- sort(abs(pca$rotation[,1]),decreasing = TRUE)
top_5_genes <- names(gene_score[1:5])
top_5_genes

pca$rotation[top_5_genes,1]
```



## Hand-on Worksheet Part 2 
```{r}
food_data <- read.csv(file = "UK_foods.csv",row.names = 1)
## Complete the following code to find out how many rows and columns are in x?
dim(food_data)
## Preview the first 6 rows
head(food_data,6)
```

```{r}
barplot(as.matrix(food_data), beside=T, col=rainbow(nrow(food_data)))
```

```{r}
barplot(as.matrix(food_data), beside=F, col=rainbow(nrow(food_data)))
```

```{r}
pairs(food_data, col=rainbow(10), pch=16)
```
```{r}
# Use the prcomp() PCA function 
pca <- prcomp( t(food_data) )
summary(pca)
```
```{r}
# Plot PC1 vs PC2
col <-  c("orange","red","blue","green")
plot(pca$x[,1], pca$x[,2], xlab="PC1", ylab="PC2", xlim=c(-270,500))
text(pca$x[,1], pca$x[,2], colnames(food_data),col = col)

```
```{r}
v <- round( pca$sdev^2/sum(pca$sdev^2) * 100 )
v
## or the second row here...
z <- summary(pca)
z$importance
barplot(v, xlab="Principal Component", ylab="Percent Variation")
```
```{r}
## Lets focus on PC1 as it accounts for > 90% of variance 
par(mar=c(10, 3, 0.35, 0))
barplot( pca$rotation[,1], las=2 )
```

